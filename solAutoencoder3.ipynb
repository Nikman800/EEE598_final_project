{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bae1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Setup for SOL Supercomputer (Headless Mode)\n",
    "# solAutoencoder3: Enhanced SimpleUNet + 256x256 patches + Overlapping tiles + Nodata masking\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "# Files\n",
    "l1_file = \"satellite_data/LC08_L1TP_037037_20250603_20250607_02_T1_combined.tif\"\n",
    "l2_file = \"satellite_data/LC08_L2SP_037037_20250603_20250607_02_T1_combined.tif\"\n",
    "\n",
    "L2_THERMAL_BAND_INDEX = 6  \n",
    "\n",
    "# CONFIG - Optimized for accuracy\n",
    "PATCH_SIZE = 256       # Smaller patches = more training samples\n",
    "TILE_STRIDE = 128      # 50% overlap during tiling = 4x more samples\n",
    "BATCH_SIZE = 16        \n",
    "EPOCHS = 150           \n",
    "NUM_WORKERS = 8        \n",
    "LR = 1e-4              \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Data & Normalize\n",
    "\n",
    "with rasterio.open(l1_file) as src1:\n",
    "    L1_all = src1.read().astype(np.float32)\n",
    "    l1_profile = src1.profile\n",
    "\n",
    "with rasterio.open(l2_file) as src2:\n",
    "    L2 = src2.read(L2_THERMAL_BAND_INDEX).astype(np.float32)\n",
    "    l2_profile = src2.profile\n",
    "\n",
    "# Create nodata mask (IMPORTANT: will be used during inference)\n",
    "nodata_mask = (L2 > 0)\n",
    "print(f\"Valid pixels: {np.sum(nodata_mask):,} / {nodata_mask.size:,} ({100*np.sum(nodata_mask)/nodata_mask.size:.1f}%)\")\n",
    "\n",
    "# Normalize L1\n",
    "L1_norm = np.empty_like(L1_all, dtype=np.float32)\n",
    "for b in range(L1_all.shape[0]):\n",
    "    band = L1_all[b]\n",
    "    mask = (band > 0) & nodata_mask\n",
    "    p1, p99 = np.percentile(band[mask], [1, 99])\n",
    "    L1_norm[b] = np.clip((band - p1) / (p99 - p1), 0, 1)\n",
    "\n",
    "# Normalize L2\n",
    "L2_masked = L2[nodata_mask]\n",
    "L2_p1, L2_p99 = np.percentile(L2_masked, [1, 99])\n",
    "L2_norm = np.clip((L2 - L2_p1) / (L2_p99 - L2_p1), 0, 1)\n",
    "\n",
    "print(\"Data loaded and normalized.\")\n",
    "print(f\"L2 normalization range: {L2_p1:.2f} K to {L2_p99:.2f} K\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50952d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Overlapping Tiling (256x256 with 128 stride = 4x more samples)\n",
    "\n",
    "def tile_multiband_overlapping(img_3d, target_2d, valid_mask_2d, patch=PATCH_SIZE, stride=TILE_STRIDE):\n",
    "    \"\"\"\n",
    "    Create overlapping tiles for training.\n",
    "    With stride = patch/2, each pixel appears in up to 4 different tiles.\n",
    "    \"\"\"\n",
    "    C, H, W = img_3d.shape\n",
    "    tiles_x, tiles_y = [], []\n",
    "    \n",
    "    for i in range(0, H - patch + 1, stride):\n",
    "        for j in range(0, W - patch + 1, stride):\n",
    "            x_patch = img_3d[:, i:i+patch, j:j+patch]\n",
    "            y_patch = target_2d[i:i+patch, j:j+patch]\n",
    "            mask_patch = valid_mask_2d[i:i+patch, j:j+patch]\n",
    "            \n",
    "            # Require at least 50% valid pixels (stricter than before)\n",
    "            valid_ratio = np.mean(mask_patch)\n",
    "            if valid_ratio < 0.5:\n",
    "                continue\n",
    "            \n",
    "            tiles_x.append(x_patch)\n",
    "            tiles_y.append(y_patch)\n",
    "    \n",
    "    X = np.stack(tiles_x, axis=0)\n",
    "    Y = np.stack(tiles_y, axis=0)[:, None, ...]\n",
    "    return X, Y\n",
    "\n",
    "X_tiles, Y_tiles = tile_multiband_overlapping(L1_norm, L2_norm, nodata_mask, PATCH_SIZE, TILE_STRIDE)\n",
    "print(f\"Generated {X_tiles.shape[0]} overlapping tiles of size {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"(Compare: non-overlapping would give ~{(7891//PATCH_SIZE)*(7751//PATCH_SIZE)} tiles)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset with Augmentation\n",
    "\n",
    "class LandsatMultiBandDataset(Dataset):\n",
    "    def __init__(self, X, Y, augment=False):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.X[idx], self.Y[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if torch.rand(1) > 0.5:\n",
    "                x = torch.flip(x, [-1])\n",
    "                y = torch.flip(y, [-1])\n",
    "            # Random vertical flip\n",
    "            if torch.rand(1) > 0.5:\n",
    "                x = torch.flip(x, [-2])\n",
    "                y = torch.flip(y, [-2])\n",
    "            # Random rotation\n",
    "            k = torch.randint(0, 4, (1,)).item()\n",
    "            if k > 0:\n",
    "                x = torch.rot90(x, k, [-2, -1])\n",
    "                y = torch.rot90(y, k, [-2, -1])\n",
    "\n",
    "        return x, y\n",
    "\n",
    "full_indices = np.arange(len(X_tiles))\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(full_indices, size=int(0.8 * len(full_indices)), replace=False)\n",
    "val_indices = np.setdiff1d(full_indices, train_indices)\n",
    "\n",
    "train_ds = LandsatMultiBandDataset(X_tiles[train_indices], Y_tiles[train_indices], augment=True)\n",
    "val_ds   = LandsatMultiBandDataset(X_tiles[val_indices], Y_tiles[val_indices], augment=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train tiles: {len(train_ds)}, Val tiles: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4be99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Enhanced SimpleUNet Model (Wider + Deeper)\n",
    "\n",
    "class EnhancedSimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (wider filters: 32, 64, 128, 256)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "        \n",
    "        b = self.bottleneck(p3)\n",
    "        \n",
    "        u3 = self.up3(b)\n",
    "        c3 = torch.cat([u3, e3], dim=1)\n",
    "        d3 = self.dec3(c3)\n",
    "        \n",
    "        u2 = self.up2(d3)\n",
    "        c2 = torch.cat([u2, e2], dim=1)\n",
    "        d2 = self.dec2(c2)\n",
    "        \n",
    "        u1 = self.up1(d2)\n",
    "        c1 = torch.cat([u1, e1], dim=1)\n",
    "        out = self.dec1(c1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = EnhancedSimpleUNet(in_channels=6, out_channels=1).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model initialized: EnhancedSimpleUNet\")\n",
    "print(f\"Total parameters: {total_params:,} (~{total_params/1e6:.2f}M)\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "criterion = nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5cce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Loop\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_dl):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_dl)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            val_loss += criterion(pred, y).item()\n",
    "            \n",
    "    val_loss /= len(val_dl)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f} (Time: {time.time()-start_time:.1f}s)\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_sol3_model.pth\")\n",
    "        print(f\"  >>> Saved new best model (Val Loss: {val_loss:.6f})\")\n",
    "\n",
    "print(f\"Training complete. Total time: {time.time()-start_time:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a77699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save Learning Curve Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('L1 Loss')\n",
    "plt.title('Training History (EnhancedSimpleUNet + Overlapping Tiles)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"sol3_training_history.png\") \n",
    "print(\"Saved learning curve to sol3_training_history.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Overlapping Inference WITH Nodata Masking\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_sol3_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "C, H, W = L1_norm.shape\n",
    "stride = PATCH_SIZE // 2  # 50% overlap for smooth predictions\n",
    "\n",
    "pred_sum = np.zeros((H, W), dtype=np.float32)\n",
    "pred_count = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "n_rows = int(np.ceil((H - PATCH_SIZE) / stride) + 1)\n",
    "n_cols = int(np.ceil((W - PATCH_SIZE) / stride) + 1)\n",
    "\n",
    "print(\"Starting full-scene inference with nodata masking...\")\n",
    "with torch.no_grad():\n",
    "    for r in range(n_rows):\n",
    "        if r % 10 == 0:\n",
    "            print(f\"Processing row {r}/{n_rows}...\")\n",
    "            \n",
    "        for c in range(n_cols):\n",
    "            y0 = min(r * stride, H - PATCH_SIZE)\n",
    "            x0 = min(c * stride, W - PATCH_SIZE)\n",
    "            \n",
    "            x_patch = L1_norm[:, y0:y0+PATCH_SIZE, x0:x0+PATCH_SIZE]\n",
    "            x_tensor = torch.from_numpy(x_patch[None, ...]).float().to(device)\n",
    "            \n",
    "            y_pred = model(x_tensor).cpu().numpy()[0, 0]\n",
    "            \n",
    "            pred_sum[y0:y0+PATCH_SIZE, x0:x0+PATCH_SIZE] += y_pred\n",
    "            pred_count[y0:y0+PATCH_SIZE, x0:x0+PATCH_SIZE] += 1.0\n",
    "\n",
    "# Average predictions\n",
    "pred_norm_full = np.zeros_like(pred_sum)\n",
    "valid_pred = pred_count > 0\n",
    "pred_norm_full[valid_pred] = pred_sum[valid_pred] / pred_count[valid_pred]\n",
    "\n",
    "# De-normalize to Kelvin\n",
    "pred_kelvin_full = pred_norm_full * (L2_p99 - L2_p1) + L2_p1\n",
    "\n",
    "# IMPORTANT: Apply nodata mask - set invalid pixels to 0 (nodata)\n",
    "pred_kelvin_full[~nodata_mask] = 0\n",
    "print(f\"Applied nodata mask: {np.sum(~nodata_mask):,} pixels set to nodata\")\n",
    "\n",
    "# Clip predictions to reasonable range (avoids extreme outliers)\n",
    "valid_pred_mask = nodata_mask & (pred_kelvin_full > 0)\n",
    "pred_kelvin_full = np.clip(pred_kelvin_full, L2_p1 - 20, L2_p99 + 20)\n",
    "pred_kelvin_full[~nodata_mask] = 0  # Re-apply nodata after clipping\n",
    "\n",
    "# Save TIF\n",
    "pred_profile = l2_profile.copy()\n",
    "pred_profile.update(dtype=rasterio.float32, count=1, nodata=0)\n",
    "\n",
    "with rasterio.open(\"sol3_prediction.tif\", \"w\", **pred_profile) as dst:\n",
    "    dst.write(pred_kelvin_full.astype(np.float32), 1)\n",
    "    \n",
    "print(\"Saved: sol3_prediction.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Comprehensive Metrics Calculation\n",
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE ACCURACY METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ground truth in Kelvin\n",
    "true_kelvin = L2\n",
    "pred_kelvin = pred_kelvin_full\n",
    "\n",
    "# Create mask for valid pixels (both true and predicted must be valid)\n",
    "mask = nodata_mask & (pred_kelvin > 0)\n",
    "n_valid = np.sum(mask)\n",
    "\n",
    "# Extract valid pixel values\n",
    "true_vals = true_kelvin[mask]\n",
    "pred_vals = pred_kelvin[mask]\n",
    "error = pred_vals - true_vals\n",
    "abs_error = np.abs(error)\n",
    "\n",
    "# =========================\n",
    "# 1. BASIC ERROR METRICS\n",
    "# =========================\n",
    "mae = np.mean(abs_error)\n",
    "rmse = np.sqrt(np.mean(error**2))\n",
    "max_error = np.max(abs_error)\n",
    "min_error = np.min(abs_error)\n",
    "\n",
    "print(f\"\\n1. BASIC ERROR METRICS:\")\n",
    "print(f\"   MAE (Mean Absolute Error):    {mae:.3f} K\")\n",
    "print(f\"   RMSE (Root Mean Square Error): {rmse:.3f} K\")\n",
    "print(f\"   Max Absolute Error:            {max_error:.3f} K\")\n",
    "print(f\"   Min Absolute Error:            {min_error:.3f} K\")\n",
    "\n",
    "# =========================\n",
    "# 2. BIAS METRICS\n",
    "# =========================\n",
    "bias = np.mean(error)\n",
    "median_error = np.median(error)\n",
    "\n",
    "print(f\"\\n2. BIAS METRICS:\")\n",
    "print(f\"   Mean Bias (ME):     {bias:+.3f} K {'(over-predicting)' if bias > 0 else '(under-predicting)'}\")\n",
    "print(f\"   Median Error:       {median_error:+.3f} K\")\n",
    "\n",
    "# =========================\n",
    "# 3. CORRELATION METRICS\n",
    "# =========================\n",
    "ss_res = np.sum(error**2)\n",
    "ss_tot = np.sum((true_vals - np.mean(true_vals))**2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "correlation = np.corrcoef(true_vals, pred_vals)[0, 1]\n",
    "\n",
    "print(f\"\\n3. CORRELATION METRICS:\")\n",
    "print(f\"   R2 Score:           {r2:.5f} ({r2*100:.2f}% variance explained)\")\n",
    "print(f\"   Pearson Correlation: {correlation:.5f}\")\n",
    "\n",
    "# =========================\n",
    "# 4. PERCENTILE ERROR METRICS\n",
    "# =========================\n",
    "p50 = np.percentile(abs_error, 50)\n",
    "p90 = np.percentile(abs_error, 90)\n",
    "p95 = np.percentile(abs_error, 95)\n",
    "p99 = np.percentile(abs_error, 99)\n",
    "\n",
    "print(f\"\\n4. PERCENTILE ERROR DISTRIBUTION:\")\n",
    "print(f\"   50th percentile (median): {p50:.3f} K\")\n",
    "print(f\"   90th percentile:          {p90:.3f} K\")\n",
    "print(f\"   95th percentile:          {p95:.3f} K\")\n",
    "print(f\"   99th percentile:          {p99:.3f} K\")\n",
    "\n",
    "# =========================\n",
    "# 5. ERROR THRESHOLDS\n",
    "# =========================\n",
    "within_1K = np.sum(abs_error <= 1.0) / n_valid * 100\n",
    "within_2K = np.sum(abs_error <= 2.0) / n_valid * 100\n",
    "within_5K = np.sum(abs_error <= 5.0) / n_valid * 100\n",
    "\n",
    "print(f\"\\n5. ERROR THRESHOLDS (% of pixels within error):\")\n",
    "print(f\"   Within +/-1 K:  {within_1K:.2f}%\")\n",
    "print(f\"   Within +/-2 K:  {within_2K:.2f}%\")\n",
    "print(f\"   Within +/-5 K:  {within_5K:.2f}%\")\n",
    "\n",
    "# =========================\n",
    "# 6. TEMPERATURE RANGE ANALYSIS\n",
    "# =========================\n",
    "print(f\"\\n6. TEMPERATURE RANGE ANALYSIS:\")\n",
    "print(f\"   True temp range:      {true_vals.min():.2f} K to {true_vals.max():.2f} K\")\n",
    "print(f\"   Predicted temp range: {pred_vals.min():.2f} K to {pred_vals.max():.2f} K\")\n",
    "print(f\"   True mean temp:       {true_vals.mean():.2f} K\")\n",
    "print(f\"   Predicted mean temp:  {pred_vals.mean():.2f} K\")\n",
    "\n",
    "# =========================\n",
    "# 7. SUMMARY SCORE\n",
    "# =========================\n",
    "accuracy_score = (r2 * 0.4) + ((1 - mae/10) * 0.3) + (within_2K/100 * 0.3)\n",
    "print(f\"\\n7. OVERALL ACCURACY SCORE: {accuracy_score:.4f} (0-1 scale, higher is better)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Save metrics to file\n",
    "with open(\"sol3_metrics.txt\", \"w\") as f:\n",
    "    f.write(\"COMPREHENSIVE ACCURACY METRICS (solAutoencoder3)\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"MAE: {mae:.3f} K\\n\")\n",
    "    f.write(f\"RMSE: {rmse:.3f} K\\n\")\n",
    "    f.write(f\"R2 Score: {r2:.5f}\\n\")\n",
    "    f.write(f\"Bias (ME): {bias:+.3f} K\\n\")\n",
    "    f.write(f\"Correlation: {correlation:.5f}\\n\")\n",
    "    f.write(f\"Max Error: {max_error:.3f} K\\n\")\n",
    "    f.write(f\"Within +/-1K: {within_1K:.2f}%\\n\")\n",
    "    f.write(f\"Within +/-2K: {within_2K:.2f}%\\n\")\n",
    "    f.write(f\"Within +/-5K: {within_5K:.2f}%\\n\")\n",
    "    f.write(f\"Accuracy Score: {accuracy_score:.4f}\\n\")\n",
    "    \n",
    "print(\"Saved metrics to sol3_metrics.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Generate Comprehensive Comparison Plot\n",
    "print(\"Generating comparison plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Row 1: Temperature maps\n",
    "vmin = np.percentile(true_kelvin[mask], 2)\n",
    "vmax = np.percentile(true_kelvin[mask], 98)\n",
    "err_max = np.percentile(np.abs(error), 98)\n",
    "\n",
    "# True Temperature\n",
    "im0 = axes[0, 0].imshow(true_kelvin, cmap='inferno', vmin=vmin, vmax=vmax)\n",
    "axes[0, 0].set_title(\"True L2 Temperature (K)\", fontsize=12)\n",
    "plt.colorbar(im0, ax=axes[0, 0], label='K')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Predicted Temperature\n",
    "im1 = axes[0, 1].imshow(pred_kelvin, cmap='inferno', vmin=vmin, vmax=vmax)\n",
    "axes[0, 1].set_title(\"Predicted Temperature (K)\", fontsize=12)\n",
    "plt.colorbar(im1, ax=axes[0, 1], label='K')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Error Map\n",
    "error_full = pred_kelvin - true_kelvin\n",
    "error_full[~nodata_mask] = 0  # Mask nodata in error map\n",
    "im2 = axes[0, 2].imshow(error_full, cmap='bwr', vmin=-err_max, vmax=err_max)\n",
    "axes[0, 2].set_title(\"Error (Pred - True) (K)\", fontsize=12)\n",
    "plt.colorbar(im2, ax=axes[0, 2], label='K')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2: Statistical plots\n",
    "# Scatter plot\n",
    "sample_idx = np.random.choice(len(true_vals), size=min(50000, len(true_vals)), replace=False)\n",
    "axes[1, 0].scatter(true_vals[sample_idx], pred_vals[sample_idx], alpha=0.1, s=1)\n",
    "axes[1, 0].plot([true_vals.min(), true_vals.max()], [true_vals.min(), true_vals.max()], 'r--', lw=2, label='Perfect Fit')\n",
    "axes[1, 0].set_xlabel('True Temperature (K)')\n",
    "axes[1, 0].set_ylabel('Predicted Temperature (K)')\n",
    "axes[1, 0].set_title(f'Scatter Plot (R2={r2:.4f})', fontsize=12)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error Histogram\n",
    "axes[1, 1].hist(error, bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', label='Zero Error')\n",
    "axes[1, 1].axvline(x=bias, color='g', linestyle='-', label=f'Bias={bias:.2f}K')\n",
    "axes[1, 1].set_xlabel('Error (K)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Error Distribution', fontsize=12)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics Summary\n",
    "metrics_text = f'''\n",
    "METRICS SUMMARY (sol3)\n",
    "------------------------\n",
    "MAE:        {mae:.3f} K\n",
    "RMSE:       {rmse:.3f} K\n",
    "R2 Score:   {r2:.5f}\n",
    "Bias:       {bias:+.3f} K\n",
    "Correlation:{correlation:.5f}\n",
    "\n",
    "Within +/-1K: {within_1K:.1f}%\n",
    "Within +/-2K: {within_2K:.1f}%\n",
    "Within +/-5K: {within_5K:.1f}%\n",
    "\n",
    "90th %ile:  {p90:.3f} K\n",
    "95th %ile:  {p95:.3f} K\n",
    "'''\n",
    "axes[1, 2].text(0.1, 0.5, metrics_text, transform=axes[1, 2].transAxes, \n",
    "                fontsize=12, verticalalignment='center', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "axes[1, 2].axis('off')\n",
    "axes[1, 2].set_title('Metrics Summary', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sol3_comparison_output.png\", dpi=150)\n",
    "print(\"Saved comparison plot to sol3_comparison_output.png\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
